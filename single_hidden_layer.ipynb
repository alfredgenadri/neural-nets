{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSingle hidden layer neural network\\n\\nA[1] = relu(W[1].T dot X + B[1])\\nA[2] = sigmoid(W[2].T dot A[1] + B[2])\\n\\nwhere X[1]: (n_x, n_examples), W[1]: (n_x, n_hidden), B[1]: (n_hidden, n_examples)\\nand A[1]: (n_hidden, n_examples), W[2]: (n_hidden, n_y), B[2]: (n_y, n_examples)\\n\\nConsidering the n_examples can vary from training to testing, the biases will be broadcasted\\n\\nFurther inquiries:\\nUnderstand relation between input topologies and network architecture\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Single hidden layer neural network\n",
    "\n",
    "A[1] = relu(W[1].T dot X + B[1])\n",
    "A[2] = sigmoid(W[2].T dot A[1] + B[2])\n",
    "\n",
    "where X[1]: (n_x, n_examples), W[1]: (n_x, n_hidden), B[1]: (n_hidden, n_examples)\n",
    "and A[1]: (n_hidden, n_examples), W[2]: (n_hidden, n_y), B[2]: (n_y, n_examples)\n",
    "\n",
    "Considering the n_examples can vary from training to testing, the biases will be broadcasted\n",
    "\n",
    "Further inquiries:\n",
    "Understand relation between input topologies and network architecture\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input dimensions: (12288, 209)\n",
      "Testing input dimensions: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "\n",
    "    # load training and test data: pixel values for cat images and binary classification output\n",
    "\n",
    "    with h5py.File('datasets/train_catvnoncat.h5', \"r\") as train_dataset:\n",
    "        train_x_original = np.array(train_dataset[\"train_set_x\"][:])\n",
    "        train_y_original = np.array(train_dataset[\"train_set_y\"][:])\n",
    "\n",
    "    with h5py.File('datasets/test_catvnoncat.h5', \"r\") as test_dataset:\n",
    "        test_x_original = np.array(test_dataset[\"test_set_x\"][:])\n",
    "        test_y_original = np.array(test_dataset[\"test_set_y\"][:])\n",
    "        classes = np.array(test_dataset[\"list_classes\"][:])\n",
    "\n",
    "    # output matrix of the form (1, n) where each column is a boolean \n",
    "    train_y_original = train_y_original.reshape((1, train_y_original.shape[0]))\n",
    "    test_y_original = test_y_original.reshape((1, test_y_original.shape[0]))\n",
    "\n",
    "    return train_x_original, train_y_original, test_x_original, test_y_original, classes\n",
    "\n",
    "train_x_original, train_y_original, test_x_original, test_y_original, classes = load_dataset()\n",
    "\n",
    "n_train = train_x_original.shape[0] # number of training examples\n",
    "n_test = test_x_original.shape[0] # number of test examples\n",
    "num_pixels = train_x_original.shape[1] # number of pixels = height = width\n",
    "img_shape = (num_pixels, num_pixels, train_x_original.shape[3]) # height, width, 3 channels (RGB)\n",
    "\n",
    "# flatten data and standardize pixel values\n",
    "# rows represent pixels and columns different training examples\n",
    "\n",
    "train_x_flatten = train_x_original.reshape(n_train, -1).T\n",
    "test_x_flatten = test_x_original.reshape(n_test, -1).T\n",
    "\n",
    "print('Training input dimensions: ' + str(train_x_flatten.shape))\n",
    "print('Testing input dimensions: ' + str(test_x_flatten.shape))\n",
    "\n",
    "# distribute the data over the range 0-1\n",
    "train_x = train_x_flatten / 255\n",
    "test_x = test_x_flatten / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 12288\n",
    "n_hidden = 4\n",
    "n_y = 1\n",
    "layer_sizes = [n_x, n_hidden, n_y]\n",
    "n_layers = len(layer_sizes) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_params(layer_sizes):\n",
    "    params = dict()\n",
    "    for l in range(1, len(layer_sizes)):\n",
    "        params['W{layer}'.format(layer=l)] = np.random.randn(layer_sizes[l - 1],layer_sizes[l]) * np.sqrt(2/layer_sizes[l-1]) # He et al. initialization\n",
    "        params['b{layer}'.format(layer=l)] = np.zeros((layer_sizes[l], 1))\n",
    "    return params\n",
    "\n",
    "def forward_prop(params, X):\n",
    "    cache = dict()\n",
    "    cache['A0'] = X\n",
    "    for l in range(1, n_layers + 1):\n",
    "        Z_l = np.dot(params['W{layer}'.format(layer=l)].T, cache['A{prev_l}'.format(prev_l=l-1)]) + params['b{layer}'.format(layer=l)]\n",
    "        cache['A{layer}'.format(layer=l)] = tanh(Z_l) if l != n_layers else sigmoid(Z_l)\n",
    "    return params, cache\n",
    "\n",
    "def compute_cost(Ak, Y):\n",
    "    cost = -np.sum(np.multiply(np.log(Ak), Y) + np.multiply((1 - Y), np.log(1 - Ak))) / Y.shape[1]\n",
    "    return np.squeeze(cost)\n",
    "\n",
    "def backward_prop(params, cache, X, Y):\n",
    "\n",
    "    n = X.shape[1]\n",
    "    dZ2 = Y - cache['A2']\n",
    "    dW2 = (-1/n) * np.dot(dZ2, cache['A1'].T).T #.T\n",
    "    db2 = (-1/n) * np.sum(dZ2, axis=1, keepdims=1)\n",
    "    temp = 1 - np.power(cache['A1'], 2)\n",
    "    dZ1 = np.multiply(np.dot(params['W2'], dZ2), 1 - np.power(cache['A1'], 2))\n",
    "    dW1 = (-1/n) * np.dot(dZ1, cache['A0'].T).T\n",
    "    db1 = (-1/n) * np.sum(dZ1, axis=1, keepdims=1)\n",
    "\n",
    "    return {'dW2': dW2,\n",
    "            'db2': db2,\n",
    "            'dW1': dW1,\n",
    "            'db1': db1 }\n",
    "\n",
    "def optimize(params, grads, learning_rate):\n",
    "    for param, val in params.items():\n",
    "        params[param] = params[param] - learning_rate * grads['d' + param]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, X):\n",
    "    n = X.shape[1]\n",
    "    _, cache = forward_prop(params, X)\n",
    "    Y_pred = np.zeros((1, n))\n",
    "    \n",
    "    for idx in range(n):\n",
    "        Y_pred[0, idx] = 1 if cache['A{0}'.format(n_layers)][0, idx] > 0.5 else 0 # thresholding\n",
    "    return Y_pred\n",
    "    \n",
    "def model(learning_rate, X_train, Y_train, X_test, Y_test, epoch_num=15000):\n",
    "    \n",
    "    params = init_params(layer_sizes)\n",
    "    for epoch in range(epoch_num):\n",
    "        params, cache = forward_prop(params, X_train)\n",
    "        cost = compute_cost(cache['A2'], Y_train)\n",
    "        grads = backward_prop(params, cache, X_train, Y_train)\n",
    "        params = optimize(params, grads, learning_rate)\n",
    "        if (epoch % 1000 == 0):\n",
    "            print('Cost after iteration {iteration}: {cost}'.format(iteration=epoch, cost=cost)) \n",
    "    Y_pred_train = predict(params, X_train)\n",
    "    Y_pred_test = predict(params, X_test)\n",
    "    test_acc = 100 - np.mean(np.abs(Y_pred_test - Y_test)) * 100\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_pred_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(test_acc))\n",
    "    return params, test_acc\n",
    "\n",
    "m = model(0.001, train_x, train_y_original, test_x, test_y_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]\n",
    "best_model = m\n",
    "for lr in learning_rates:\n",
    "    best_model = max(best_model, model(lr, train_x, train_y_original, test_x, test_y_original), key=lambda m: m[1])\n",
    "print(best_model[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same architecture using PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "m = nn.Sequential(nn.Linear(12288, 4),\n",
    "   nn.Tanh(),\n",
    "   nn.Linear(4, 1),\n",
    "   nn.Sigmoid())\n",
    "\n",
    "# hyperparams\n",
    "epochs = 10000\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "# loss fxn and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(m.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# Gradient Descent\n",
    "for epoch in range(epochs):\n",
    "   # Forward pass: Compute predicted y by passing x to the model\n",
    "   y_pred = m(torch.from_numpy(train_x.T.astype(np.float32)))\n",
    "   # Compute and print loss\n",
    "   loss = criterion(y_pred, torch.from_numpy(train_y_original.T.astype(np.float32)))\n",
    "   print('epoch: ', epoch,' loss: ', loss.item())\n",
    "   # Zero gradients, perform a backward pass, and update the weights.\n",
    "   optimizer.zero_grad()\n",
    "   # perform a backward pass (backpropagation)\n",
    "   loss.backward()\n",
    "   # Update the parameters\n",
    "   optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.0\n"
     ]
    }
   ],
   "source": [
    "def predict(model, x, y):\n",
    "    output = m(torch.from_numpy(x.T.astype(np.float32)))\n",
    "    Y_pred = np.zeros(y.shape)\n",
    "    for idx in range(Y_pred.shape[1]):\n",
    "        Y_pred[0, idx] = 1 if output[idx, 0] > 0.5 else 0 # thresholding\n",
    "    return Y_pred\n",
    "\n",
    "pr = predict(m, test_x, test_y_original)\n",
    "print(100 - np.mean(np.abs(pr - test_y_original)) * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
