{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Single hidden layer neural network\n",
    "\n",
    "A[1] = relu(W[1].T dot X + B[1])\n",
    "A[2] = sigmoid(W[2].T dot A[1] + B[2])\n",
    "\n",
    "where X[1]: (n_x, n_examples), W[1]: (n_x, n_hidden), B[1]: (n_hidden, n_examples)\n",
    "and A[1]: (n_hidden, n_examples), W[2]: (n_hidden, n_y), B[2]: (n_y, n_examples)\n",
    "\n",
    "Considering the n_examples can vary from training to testing, the biases will be broadcasted\n",
    "\n",
    "Further inquiries:\n",
    "Understand relation between input topologies and network architecture\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 12288\n",
    "n_hidden = 4\n",
    "n_y = 1\n",
    "layer_sizes = [n_x, n_hidden, n_y]\n",
    "n_layers = len(layer_sizes) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return max(0, z)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_params(layer_sizes):\n",
    "\n",
    "    params = dict()\n",
    "    for l in range(1, len(layer_sizes)):\n",
    "        params['W{layer}'.format(layer=l)] = np.random.randn(layer_sizes[l - 1],layer_sizes[l]) * np.sqrt(2/layer_sizes[l-1]) # He et al. initialization\n",
    "        params['B{layer}'.format(layer=l)] = np.zeros((layer_sizes[l]))\n",
    "    \n",
    "    return params\n",
    "\n",
    "def forward_prop(params, X, n_layers):\n",
    "    \n",
    "    cache = dict()\n",
    "    cache['A0'] = X\n",
    "    for l in range(1, n_layers + 1):\n",
    "        Z_l = np.dot(params['W{layer}'.format(layer=l)].T, cache['A{prev_l}'.format(prev_l=l-1)]) + params['B{layer}'.format(layer=l)]\n",
    "        cache['A{layer}'.format(layer=l)] = relu(Z_l) if l != n_layers else sigmoid(Z_l)\n",
    "    return params, cache\n",
    "\n",
    "def backward_prop():\n",
    "    pass\n",
    "\n",
    "def optimize():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    pass\n",
    "\n",
    "def model():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
